<h1 align="center"> Voice-Driven Mood Detection & Entertainment Recommendation</h1>
<p align="center">
  <img src="https://img.shields.io/badge/Deep%20Learning-PyTorch-red" alt="PyTorch Badge">
  <img src="https://img.shields.io/badge/Model-CNN%20%2B%20Wav2Vec2-blue" alt="Model">
  <img src="https://img.shields.io/badge/Recommendation-System-green" alt="Recommendation">
</p>

<p align="center">
  <em>A full-stack AI system that listens to your voice, detects your mood, and recommends the perfect entertainment content ğŸµğŸ¬</em>
</p>

---

## ğŸ§  Overview

This project integrates **emotion recognition** and **personalized content recommendation** into a seamless pipeline. We first detect emotional tone from user speech using a **Wav2Vec2 + CNN model**, then recommend content (e.g., music, videos) based on detected emotions.

---

## ğŸ¯ Features

- ğŸ¤ **Voice Emotion Detection**: Identify emotional tone (Happy, Sad, Angry, Neutral, Surprise)
- ğŸ§¬ **Deep Learning Pipeline**: Wav2Vec2 feature extraction + CNN classifier
- ğŸ¬ **Mood-Based Recommendation**: Suggests YouTube or Spotify content based on emotion
- ğŸ“Š **Model Evaluation**: Uses CrossEntropy loss, accuracy metrics
- â˜ï¸ **Scalable Deployment Ready**: Future extensions include real-time apps or browser add-ons

---

## ğŸ“ Dataset: Emotional Speech Data (ESD)

- **Link**: [GitHub - HLTSingapore/Emotional-Speech-Data](https://github.com/HLTSingapore/Emotional-Speech-Data)
- **Languages**: Multilingual (we used 3 English speakers)
- **Classes**: Neutral, Happy, Sad, Angry, Surprise
- **Format**: `.wav` audio clips with labeled emotions

> **Citation**: Kun Zhou et al., "Emotional voice conversion: Theory, databases and ESD", *Speech Communication*, 2022

---

## ğŸ§ª Methodology

### ğŸ”¹ Audio Preprocessing
- Used Hugging Faceâ€™s **Wav2Vec2** to extract audio embeddings of shape `[batch, seq_len, 768]`
- Handled variable-length sequences via custom `collate_fn`
- Batched data with PyTorch's `DataLoader`

### ğŸ”¹ Model Architecture: CNN on Wav2Vec2

```text
[768 features] â†’
â†’ Conv1D(256) â†’ BatchNorm â†’ ReLU â†’ Pool â†’
â†’ Conv1D(128) â†’ BatchNorm â†’ ReLU â†’ Pool â†’
â†’ Conv1D(64) â†’ BatchNorm â†’ ReLU â†’ Pool â†’
â†’ Flatten â†’ FC(128) â†’ Dropout â†’ FC(5 classes)
```

- **Loss**: `CrossEntropyLoss`
- **Optimizer**: `SGD`
- **Classes**: 5 (Neutral, Happy, Sad, Angry, Surprise)

### ğŸ”¹ Training Strategy

- **Train/Val/Test Split**: 70/15/15
- Monitored loss and accuracy on validation set
- Final model ready for real-time predictions

---

## ğŸ“Š Results

| Metric   | Value     |
|----------|-----------|
| Accuracy | ~85â€“90%   |
| Classes  | 5         |
| Framework| PyTorch   |

- Validation results show strong generalization despite speaker variation
- Emotion classification performance varies across categories (e.g., Sad vs. Surprise)

---

## ğŸ§ Recommendation System

Once emotion is detected, the system maps it to a type of content:

| Emotion  | Recommendation                    |
|----------|------------------------------------|
| Happy    | ğŸ¶ Upbeat songs / comedy videos    |
| Sad      | ğŸ» Comforting music / calm videos  |
| Angry    | ğŸ§˜ Meditation / nature documentaries |
| Surprise | ğŸ§  Curious facts / light humor     |
| Neutral  | ğŸŒŸ Top charts / trending content   |

Future integrations could link real-time results to:
- **YouTube API**
- **Spotify API**
- **Tidal, Netflix**, etc.

---

## ğŸš€ Future Work

- ğŸŒ Web app or mobile app integration (e.g., Streamlit or Flask)
- ğŸ—£ï¸ Add multilingual emotion detection support
- ğŸ™ï¸ Real-time streaming audio analysis
- ğŸ”Œ Plugin for Discord/Zoom or browser extensions

---

## ğŸ–¥ï¸ How to Run

```bash
# 1. Clone this repository
git clone https://github.com/yourusername/voice-mood-detection.git
cd voice-mood-detection

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run notebook
jupyter notebook Final_Project_Report.ipynb
```

---

## ğŸ“‚ Folder Structure

```bash
â”œâ”€â”€ Final_Project_Report.ipynb       # Main notebook with full pipeline
â”œâ”€â”€ data/                            # ESD dataset folder (manual download)
â”œâ”€â”€ saved_models/                    # Trained CNN weights (optional)
â”œâ”€â”€ utils/                           # Helper functions (e.g., collate_fn, loaders)
â”œâ”€â”€ README.md                        # This fancy file
```

---

## ğŸ“¬ Contact

If you have questions, contact any team member:

- ğŸ“§ Runlu Dong â€“ runlu@example.com
- ğŸ“§ Ziyu Lyu â€“ ziyu@example.com
- ğŸ“§ Mengyang Liu â€“ mengyang@example.com

---

## â­ Acknowledgements

- ğŸ¤— Hugging Face for `Wav2Vec2`
- ğŸ“š PyTorch community
- ğŸ“Š ESD Dataset authors

---

<p align="center">
  <i>â€œYour voice tells a story â€” let AI interpret it and entertain you.â€</i>
</p>
